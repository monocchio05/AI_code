{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbd2923",
   "metadata": {},
   "source": [
    "# Tutorial of simple GAN builded in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0327967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import glob \n",
    "import imageio\n",
    "import math \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ce120",
   "metadata": {},
   "source": [
    "It’s a good practice to set up a <b>random generator seed</b> so that the experiment can be replicated identically on any machine. To do that in PyTorch, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a706ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(111)\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6540061",
   "metadata": {},
   "source": [
    "Now we preparing the training data composed as follow:\n",
    "<br>\n",
    "<img src=\"https://latex.codecogs.com/svg.image?\\left&space;({x_1}{,}{x_2}\\right&space;):\" />\n",
    "<br>\n",
    "<img src='https://latex.codecogs.com/svg.image?{x_2}{=}{sin}{x_1}'/>\n",
    "<img src='https://latex.codecogs.com/svg.image?0\\leq&space;{{x_1}\\leq2\\pi'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f704934",
   "metadata": {},
   "source": [
    "Now we create a data loader called train_loader, which will shuffle the data from train_set and return batches of 32 samples that you’ll use to train the neural networks.\n",
    "\n",
    "After setting up the training data, you need to create the neural networks for the discriminator and generator that will compose the GAN. In the following section, you’ll implement the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df200cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_length = 1024*2\n",
    "train_data = torch.zeros((train_data_length, 2))\n",
    "train_data[:,0] = 2 * math.pi * torch.rand(train_data_length)\n",
    "train_data[:,1] = torch.sin(train_data[:,0])**2\n",
    "train_labels = torch.zeros((train_data_length))\n",
    "\n",
    "train_set = [\n",
    "    (train_data[i], train_labels[i]) for i in range(train_data_length)\n",
    "]\n",
    "\n",
    "plt.plot(train_data[:, 0], train_data[:, 1], \".\")\n",
    "\n",
    "\n",
    "#plt.plot(train_set)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f771cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f2853e",
   "metadata": {},
   "source": [
    "# Discriminator implementation\n",
    "The discriminator is a model with a two-dimensional input and a one-dimensional output. It’ll receive a sample from the real data or from the generator and will provide the probability that the sample belongs to the real training data. The code below shows how to create a discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "discriminator = Discriminator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84198fe4",
   "metadata": {},
   "source": [
    "# Generator Implementation\n",
    "In generative adversarial networks, the generator is the model that takes samples from a latent space as its input and generates data resembling the data in the training set. In this case, it’s a model with a two-dimensional input, which will receive random points (z₁, z₂), and a two-dimensional output that must provide (x̃₁, x̃₂) points resembling those from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "generator = Generator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3710d9a9",
   "metadata": {},
   "source": [
    "# Training\n",
    "Now we can pass to thr taining phase chosing hyperparameter.\n",
    "<br>\n",
    "The binary cross-entropy function is a suitable loss function for training the discriminator because it considers a binary classification task. It’s also suitable for training the generator since it feeds its output to the discriminator, which provides a binary observable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 400\n",
    "loss_function = nn.BCELoss()\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "loss_d = []\n",
    "loss_g = []\n",
    "epochs_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #print(epochs_list)\n",
    "    #print(loss_d)\n",
    "    for n, (real_samples, _) in enumerate(train_loader):\n",
    "        # Data for training the discriminator\n",
    "        real_samples_labels = torch.ones((batch_size, 1))\n",
    "        latent_space_samples = torch.randn((batch_size, 2))\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        generated_samples_labels = torch.zeros((batch_size, 1))\n",
    "        all_samples = torch.cat((real_samples, generated_samples))\n",
    "        all_samples_labels = torch.cat(\n",
    "            (real_samples_labels, generated_samples_labels)\n",
    "        )\n",
    "\n",
    "        # Training the discriminator\n",
    "        discriminator.zero_grad()\n",
    "        output_discriminator = discriminator(all_samples)\n",
    "        loss_discriminator = loss_function(\n",
    "            output_discriminator, all_samples_labels)\n",
    "        loss_discriminator.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # Data for training the generator\n",
    "        latent_space_samples = torch.randn((batch_size, 2))\n",
    "\n",
    "        # Training the generator\n",
    "        generator.zero_grad()\n",
    "        generated_samples = generator(latent_space_samples)\n",
    "        output_discriminator_generated = discriminator(generated_samples)\n",
    "        loss_generator = loss_function(\n",
    "            output_discriminator_generated, real_samples_labels\n",
    "        )\n",
    "        loss_generator.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "\n",
    "        # Metrica di visualizzazione\n",
    "        if epoch % 10 == 9 and n == batch_size - 1:\n",
    "            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n",
    "            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")\n",
    "            print(type(loss_discriminator))\n",
    "            \n",
    "            fig, axs = plt.subplots(2)\n",
    "            axs[0].plot(np.array(epochs_list), loss_d)\n",
    "            axs[0].set_title('Discriminator loss')\n",
    "            axs[1].plot(np.array(epochs_list), loss_g)\n",
    "            axs[1].set_title('Generator loss')\n",
    "            #axs[2].plot(generated_samples.detach()[:,0], generated_samples.detach()[:,1], '.')\n",
    "            #axs[2].set_title('Generated Sample')\n",
    "            plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.99, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.6)\n",
    "            plt.show()\n",
    "            plt.plot(generated_samples.detach()[:,0], generated_samples.detach()[:,1], '.')\n",
    "\n",
    "    latent_space_samples = torch.randn(100, 2)\n",
    "    generated_samples = generator(latent_space_samples)\n",
    "    generated_samples = generated_samples.detach()\n",
    "    plt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")\n",
    "    plt.savefig(\"{}.png\".format(epoch))\n",
    "    plt.show()\n",
    "    with imageio.get_writer('mygif.gif', mode='I') as writer:\n",
    "        filenames = glob.glob(\"*.png\")\n",
    "        filenames = sorted(filenames)\n",
    "        for filename in filenames:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "\n",
    "    \n",
    "    epochs_list.append(epoch)\n",
    "    loss_d.append(np.array(loss_discriminator.item()))\n",
    "    loss_g.append(np.array(loss_generator.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffc5e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_space_samples = torch.randn(100, 2)\n",
    "generated_samples = generator(latent_space_samples)\n",
    "\n",
    "generated_samples = generated_samples.detach()\n",
    "plt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")\n",
    "plt.plot(train_data[:, 0], train_data[:, 1], \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5516b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51869dfec44719b3a94e73fbb5e416ae04c935be22de04176be94114cf715e5b"
  },
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
