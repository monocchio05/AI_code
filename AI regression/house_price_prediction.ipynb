{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniziamo istallando le dipendenenze che servono per runnare il seguente codice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas \n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOUSE PRICE PREDICTION\n",
    "## INTRODUCTION\n",
    "In questo tutorial vediamo come affrontare il problema della predizione del prezzo delle case partendo da un dataset che ha molte features al suo interno e in particolare, le colonne del dataset rappresentano eìle feeatures mentre le righe rappresentano il numero di case di cui abbiamo i dati.\n",
    "\n",
    "\n",
    "- Iniziamo importando le librerie necessarie per affrontare questo problema di regressione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerie per analisi del dataset \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "#data visualisation \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# per l'analisi del dataset \n",
    "import scipy\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statistics import mean\n",
    "\n",
    "# per il  machine learning\n",
    "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# per la spiegazione dei dati\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset originale descrive ogni casa con 81 features ma per semplicità noi andiamo a prendere solo 12 features, inoltre all'interno del dataset ci sono molte variabili che sono categoriche o ___NaN___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per esempio facciamo vedere quali sono tutte le colonne che compongono il df\n",
    "df_complete = pd.read_csv(\"dataset_house_price/train.csv\")\n",
    "df_complete.head()\n",
    "# opzioni per il plot\n",
    "palette = \"viridis\"\n",
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selezioniamo quindi solo le 12 colonne di nostro interesse. Per comprendere cosa sono queste features, il dataset che abbiamo scaricato da kaggle, ha un file al suo interno che si chiama _datadescription.txt_ utile proprio alla descrizione dell'intero dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = [\"Id\",\"OverallQual\",\"GrLivArea\",\"GarageCars\", \n",
    "        \"GarageArea\",\"TotalBsmtSF\",\"FullBath\",\n",
    "        \"YearBuilt\",\"YearRemodAdd\",\n",
    "        \"LotFrontage\",\"MSSubClass\", \"SalePrice\"]\n",
    "\n",
    "df_sliced = df_complete[cols]\n",
    "df_sliced.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA ANALYSIS\n",
    "Il primo passo quando ci troviamo difronte un nuovo dataset è quello di effettuare un' analisi esplorativa. Possiamo infatti suddividere l'intero lavoro di esplorazione in differenti step:\n",
    "- Analisi delle variabili che si trovano all'interno del dataset(Categoriche, NaN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrivo una funzione utile categorizzazione delle variabili \n",
    "def rec_type(dtf, col, max_types):\n",
    "    if (dtf[col].dtype == \"0\") | (dtf[col].nunique() < max_types):\n",
    "        return \"cat\"\n",
    "    else:\n",
    "        return \"num\"\n",
    "\n",
    "#utilizziamo questa funzione per plottare una heatmap\n",
    "\n",
    "dict_cols = {col:rec_type(df_sliced, col, max_types=20) for col in df_sliced.columns}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il dizionario che abbiamo definito in alto ci permette di capire quali colonne sono categoriche e quali colonne sono numeriche infatti esso restituisce in output la descrizione delle colonne nella forma col:\"num\" o col:\"cat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = df_sliced.isnull()\n",
    "for k,v in dict_cols.items():\n",
    " if v == \"num\":\n",
    "   heatmap[k] = heatmap[k].apply(lambda x: 0.5 if x is False else 1)\n",
    " else:\n",
    "   heatmap[k] = heatmap[k].apply(lambda x: 0 if x is False else 1)\n",
    "plt.style.use(\"dark_background\")\n",
    "sns.heatmap(heatmap, cbar=False).set_title('Dataset Overview')\n",
    "print(\"\\033[1;37;48m Categorical \", \"\\033[1;30;41m Numeric \", \"\\033[1;30;47m NaN \")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "g = sns.PairGrid(df_sliced)\n",
    "g.map_diag(sns.kdeplot)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_upper(sns.scatterplot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dall'analisi che abbiamo fatto sopra possiamo dure che la variabile LotFrontage contiene dei missing data, e inoltre abbiamo capito quali sono le variabili categoriche e quali sono le variabili numeriche.\n",
    "La nostra regressione deve essere messa nella forma:\n",
    "\n",
    "$SalePrice = [w][var]$\n",
    "Poichè noi vogliamo predirre il prezzo delle case, avremo un vettore dei pesi w che moltiplicato per le nostre variabili ci darà la regressione che ci pernmettrà di capire e predirre anche il prezzo di case di cui non abbiamo dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliced = df_sliced.set_index(\"Id\")\n",
    "df_sliced = df_sliced.rename(columns={\"SalePrice\":\"Y\"})\n",
    "df_sliced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il box plot ci può essere molto utile per andare a capire la distribuzione e quali sono gli outliers del nostro dataset. Altro plot di estrema importanza è la distribuzione univariata, plotatta sotto la forma di istogramma o kde function. Per cui nella sezione in basso andiamo ad analizzare a titolo di esempio solo la distribuzione e il box plot della nostra variabile ___SalesPrice___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = []\n",
    "{num_list.append(k) for k, v in dict_cols.items() if v == \"num\" and k != \"SalePrice\" and k !=\"Id\"}\n",
    "for el in num_list:\n",
    "    x = el\n",
    "    sns.displot(data=df_sliced, x=x, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per le variabili categoriche è bene andare ad aggiungere un bar plot che ci permette di capire come sono distribuite tali variabili e se ci sono eventuali altri outliers. Per cui nella sezione successiva andiamo ad implementare proprio un plot di questo tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = []\n",
    "{cat_list.append(k) for k, v in dict_cols.items() if v == \"cat\"}\n",
    "print(\"Le variabili categoriche del dataset sono le seguenti: {}\".format(cat_list))\n",
    "\n",
    "for el in cat_list:\n",
    "    x = el\n",
    "    axs = df_sliced[x].value_counts().sort_values().plot(kind=\"barh\")\n",
    "    totals= []\n",
    "    for i in axs.patches:\n",
    "        totals.append(i.get_width())\n",
    "        total = sum(totals)\n",
    "    for i in axs.patches:\n",
    "        axs.text(i.get_width()+.3, i.get_y()+.20, \n",
    "        str(round((i.get_width()/total)*100, 2))+'%', \n",
    "        fontsize=10, color='white')\n",
    "    axs.grid(axis=\"x\")\n",
    "    plt.style.use(\"dark_background\")\n",
    "    plt.suptitle(x, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo capire da questo plot che molte delle case che sono nel nostro dataset hanno o uno o due bagni,  poi ci sono degli outliers su 0 e 3 bagni, in particolare la cosa importante da capire è se queste variabili categoriche hanno un potere regressivo per il prezzo, per cui dobbiamo passare a studiare il caso della distribuzione multivariata.\n",
    "Possiamo quindi procedere in questo modo:\n",
    "- ___Split della popolazione___: La dividiamo in quattro categorie principali a seconda del numero di bagni che sono presenti all'interno del nostro dataset\n",
    "- ___Plot e confronto della densità___: andiamo a plottare le funzioni di densità di probabilità della variabile categorica, se le distribuzioni sono differenti allora la variabile ___FullBath___ ha potere predittivo, in quanto abbiamo pattern diversi di distribuzione\n",
    "- ___Raggruppare le variabili numeriche___: Raggruppiamo le variabili numeriche, nel nostro caso il prezzo di vendita in intervalli e andiamo a plottare la composizione di ogni intervallo, se la proporzione della variabile categoriche è simile in ogni intervallo allora possiamo dire che non è predittiva.\n",
    "- ___Plot dei box___: Studiamo il comportamento degli outliers andando a plottare i boxplot \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat, num = \"FullBath\", \"Y\"\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(nrows=1, ncols=3,  sharex=False, sharey=False)\n",
    "fig.suptitle(cat+\"   vs   \"+num, fontsize=10)\n",
    "            \n",
    "### distribution\n",
    "ax1.title.set_text('density')\n",
    "sns.kdeplot(data=df_sliced, x=num, palette=palette, hue=cat, ax=ax1)\n",
    "ax1.grid(True)\n",
    "### stacked\n",
    "ax2.title.set_text('bins')\n",
    "breaks = np.quantile(df_sliced[num], q=np.linspace(0,1,11))\n",
    "tmp = df_sliced.groupby([cat, pd.cut(df_sliced[num], breaks, duplicates='drop')]).size().unstack().T\n",
    "tmp = tmp[df_sliced[cat].unique()]\n",
    "tmp[\"tot\"] = tmp.sum(axis=1)\n",
    "for col in tmp.drop(\"tot\", axis=1).columns:\n",
    "     tmp[col] = tmp[col] / tmp[\"tot\"]\n",
    "tmp.drop(\"tot\", axis=1).plot(kind='bar', stacked=True, ax=ax2, legend=False, grid=True,colormap=palette)\n",
    "### boxplot   \n",
    "ax3.title.set_text('outliers')\n",
    "sns.boxplot(x=cat, y=num, data=df_sliced, ax=ax3,palette=palette)\n",
    "ax3.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dai dati che abbiamo analizzato sembra che la correlazione ci sia, in quanto le distribuzioni analizzate sono profondamente diverse. Ma quando non vogliamo fidarci dell'analisi solo osservando i dati possiamo fare una analisi apriori detta ___ANOVA___ ovvero (ANalysis Of VAriance), che ci permette di capire attraverso il _p value_ se esiste una correlazione tra le variabili. Nel caso dei bagni delle case sembra essere anche intuitivo, poichè da come vediamo nelle distribuzioni una maggiore quantità di bagni corrisponde ad una maggiore superificie della casa quindi possiamo dire che sono altamente correlate le variabili. Estendiamo il ragionamento a tutte le variabili categoriche che abbiamo nel dataset, cosi da capire quali sono correlate e quali no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in cat_list:\n",
    "    cat, num = el, \"Y\"\n",
    "    model = smf.ols(num+' ~ '+cat, data=df_sliced).fit()\n",
    "    table = sm.stats.anova_lm(model)\n",
    "    p = table[\"PR(>F)\"][0]\n",
    "    coeff, p = None, round(p, 3)\n",
    "    conclusion = \"Correlated\" if p < 0.05 else \"Non-Correlated\"\n",
    "    print(\"Anova F: the variables\", el ,\" are\", conclusion, \"(p-value: \"+str(p)+\")\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finita l'analisi delle variabili categoriche passiamo a quella delle variabili numeriche, possiamo impostare il nostro studio basandoci su due plot fondamentalmente:\n",
    "- ___Raggruppa e compara___: Possiamo raggruppare le variabili numeriche in bins e andare a confronatre il valore medio di Y in ogni intervallo. Se l'andamento per ogni bins ha pattern diversi allora potremmo dire che la variabile numerica è predittiva\n",
    "- ___Scatterplot___: Scatter plot con le distribuzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = []\n",
    "{num_list.append(k) for k, v in dict_cols.items() if v == \"num\" and k != \"SalePrice\" and k !=\"Id\"}\n",
    "print(\"Le variabili numeriche del dataset sono le seguenti: {}\".format(num_list))\n",
    "figsize = (10,5)\n",
    "\n",
    "for el in num_list:\n",
    "    x, y = el, \"Y\"\n",
    "    dtf_noNan = df_sliced[df_sliced[x].notnull()]\n",
    "    breaks = np.quantile(dtf_noNan[x], q=np.linspace(0, 1, 11))\n",
    "    groups = dtf_noNan.groupby([pd.cut(dtf_noNan[x], bins=breaks, \n",
    "               duplicates='drop')])[y].agg(['mean','median','size'])\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    fig.suptitle(x+\"   vs   \"+y, fontsize=20)\n",
    "    groups[[\"mean\", \"median\"]].plot(kind=\"line\", ax=ax)\n",
    "    groups[\"size\"].plot(kind=\"bar\", ax=ax, rot=45, secondary_y=True,\n",
    "                        color=\"grey\", alpha=0.3, grid=True)\n",
    "    ax.set(ylabel=y)\n",
    "    ax.right_ax.set_ylabel(\"Observations in each bin\")\n",
    "    plt.show()\n",
    "    sns.jointplot(x=x, y=y, data=df_sliced, dropna=True, kind='reg',height=int((figsize[0]+figsize[1])/2) )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plot sopra indicati dicono che le variabili potrebbero essere correlate tra loro, ma per andare a sfatare ogni dubbio possiamo andare a effettuare un test delle ipotesi di Pearson. Possiamo dire che attraverso questo test andiamo a misurare la correlazione che esiste tra due variabili di tipo continuo. E' importante sottolineare che la correlazione che viene misurata con questo coefficiente è solo di tipo lineare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in num_list:\n",
    "\n",
    "    x, y = el, \"Y\"\n",
    "    coeff, p = scipy.stats.pearsonr(dtf_noNan[x], dtf_noNan[y])\n",
    "    coeff, p = round(coeff, 3), round(p, 3)\n",
    "    conclusion = \"Significant\" if p < 0.05 else \"Non-Significant\"\n",
    "    print(\"Pearson Correlation of {}: \".format(el), coeff, conclusion, \"(p-value: \"+str(p)+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel dataset che abbiamo analizzato fino a questo momento abbiamo visto una classe che è MSS che ha 15 categorie, quindi andiamo a ridurre il numero di queste categorie a 3 creando dei cluster che sono, max, min e mean, cioè li andiamo a raggruppare per prezzo.\n",
    "E' stato scelto un cluster gerarchico in quanto la distribuzione dei dati non è sferica, per cui risultava poco ottimizzato utilizzare l'algoritmo KMeans, in quanto quest'ultimo lavora bene quando i dati sono distrbuiti secondo una forma sferica in cui risulta facile andare a ricavare il centroide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cluster = [\"MSSubClass\", \"SalePrice\"]\n",
    "df_cluster = df_complete[cols_cluster]\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "df_cluster.head()\n",
    "a = agg.fit(df_cluster)\n",
    "labels_a = a.labels_\n",
    "df_cluster.plot.scatter(\"MSSubClass\", \"SalePrice\", c=labels_a, colormap=palette)\n",
    "df_cluster[\"labels_a\"] = labels_a\n",
    "sns.displot(data=df_cluster, x=\"SalePrice\",kind=\"kde\",hue=\"labels_a\",palette=palette)\n",
    "df_sliced[\"MSSubClass\"] = labels_a\n",
    "print(df_sliced[\"MSSubClass\"][df_sliced.index[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING\n",
    "\n",
    "Siamo quindi arrivati alla parte di data preprocessing che possiamo suddividere in differenti fasi così riassunte:\n",
    "\n",
    "- ___Split del dataset___: Il dataset deve essere splittato in modo da avere, nella condizione più semplice dei dati di test e dei dati di train, per cui la prima azione che andremo a compiere è suddividere secondo una determinata proporzione i nostri dati. Possiamo dire in generale che ogni riga del nostro dataset rappresenterà un'_osservazione_ mentre ogni colonna rappresenta un _Features_.\n",
    "- ___Analisi dei valori___: Come sappiamo nessun dataset è perfetto per cui, prima di andare a utilizzare i nostri dati è bene capire se ci sono dei dati mancanti o NaN e andarli a sostituire con qualche altro dato.\n",
    "- ___Encoding dei dati categorici e data scaling___: I dati categorici hanno bisogno di essere encodati in quanto alcune variabili non hanno nessun significato se prese singolarmente. Inoltre è buona pratica andare a scalare i dati secondo una delle logiche utili a questo compito\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data\n",
    "dtf_train, dtf_test = model_selection.train_test_split(df_sliced, \n",
    "                      test_size=0.3)\n",
    "## print info\n",
    "print(\"X_train shape:\", dtf_train.drop(\"Y\",axis=1).shape, \"| X_test shape:\", dtf_test.drop(\"Y\",axis=1).shape)\n",
    "print(\"y_train mean:\", round(np.mean(dtf_train[\"Y\"]),2), \"| y_test mean:\", round(np.mean(dtf_test[\"Y\"]),2))\n",
    "print(dtf_train.shape[1], \"features:\", dtf_train.drop(\"Y\",axis=1).columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passiamo adesso all'analisi dei missing data, in modo da capire quali sono le colonne che hanno dei dati mancanti e li andiamo a sostituire con qualche altro valore, come la media.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dtf_train.columns.to_list()\n",
    "for el in features:\n",
    "    s = df_sliced[el].isna().sum()\n",
    "    percent = round((s*100)/(len(df_sliced.index)),2)\n",
    "    print(\"la colonna {} ha {} NaN [{} %]\".format(el,s,percent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dall'analisi sopra effettuata vediamo come l'unica variabile ad avere NaN è LotFrontage con 259 count che equivalgono al 17.74% del numero di osservazioni. Per cui su questa variabile sarà effetuata una sostituzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_train[\"LotFrontage\"] = dtf_train[\"LotFrontage\"].fillna(dtf_train[\"LotFrontage\"].mean())\n",
    "dtf_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negli algoritmi di ML è sempre utile andare a scalare il dataset sia dal lato degli input sia dal lato delle variabili di target. Per fare questo andiamo ad utilizzare la funzione scaler di ___Scikit___. In particolare andiamo ad utilizzare il robustScaler, funzione di scikit che va a rimuovere la mediama e la scala, nel IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerX = preprocessing.RobustScaler(quantile_range=(25.0, 75.0))\n",
    "X = scalerX.fit_transform(dtf_train.drop(\"Y\", axis=1))\n",
    "dtf_scaled= pd.DataFrame(X, columns=dtf_train.drop(\"Y\", \n",
    "                        axis=1).columns, index=dtf_train.index)\n",
    "## scale Y\n",
    "scalerY = preprocessing.RobustScaler(quantile_range=(25.0, 75.0))\n",
    "dtf_scaled[\"Y\"] = scalerY.fit_transform(\n",
    "                    dtf_train[\"Y\"].values.reshape(-1,1))\n",
    "dtf_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrivati a questo punto la cosa importante è andare a selezionare quali sono le features importanti per il nostro modello di ML, in particolar modo dobbiamo segliere features che vadano a prevenire il fenomeno di overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = dtf_train.corr(method=\"pearson\")\n",
    "sns.heatmap(corr_matrix, vmin=-1., vmax=1., annot=True, fmt='.2f', cmap=palette, cbar=True, linewidths=0.5)\n",
    "plt.title(\"pearson correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particolare possiamo vedere dalla matrice di correlazione che sia _GrLivArea_ sia _GarageCars_ sono altamente correlate tra loro in maniera lineare, quindi andiamo a fare una scelta tra quale delle due debba essere tenuto all'interno del modello di ML. Solitamente quando ci si trova in una situazione di questo tipo si va a selezionare la variabile con il _p-value_ più basso. Il problema che si presenta nel nostro caso viene definito problema di ___multicollinearity___ e per risolverlo dobbiamo effettuare una attenta scelta di features. Il problema sopra descritto va ad affilgere la stima dei predittori della regressione per cui deve necessariamente essere risolto. Per fare questo sono stati messi a punto molte metodologia, come ___Tikhonov regularization___ anche detta __RIDGE__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dtf_train.drop(\"Y\", axis=1).values\n",
    "y = dtf_train[\"Y\"].values\n",
    "feature_names = dtf_train.drop(\"Y\", axis=1).columns\n",
    "## p-value\n",
    "selector = feature_selection.SelectKBest(score_func=  \n",
    "               feature_selection.f_regression, k=10).fit(X,y)\n",
    "pvalue_selected_features = feature_names[selector.get_support()]\n",
    "\n",
    "## regularization\n",
    "selector = feature_selection.SelectFromModel(estimator= \n",
    "              linear_model.Ridge(alpha=1.0, fit_intercept=True), \n",
    "                                 max_features=10).fit(X,y)\n",
    "regularization_selected_features = feature_names[selector.get_support()]\n",
    " \n",
    "## plot\n",
    "dtf_features = pd.DataFrame({\"features\":feature_names})\n",
    "dtf_features[\"p_value\"] = dtf_features[\"features\"].apply(lambda x: \"p_value\" if x in pvalue_selected_features else \"\")\n",
    "dtf_features[\"num1\"] = dtf_features[\"features\"].apply(lambda x: 1 if x in pvalue_selected_features else 0)\n",
    "dtf_features[\"regularization\"] = dtf_features[\"features\"].apply(lambda x: \"regularization\" if x in regularization_selected_features else \"\")\n",
    "dtf_features[\"num2\"] = dtf_features[\"features\"].apply(lambda x: 1 if x in regularization_selected_features else 0)\n",
    "dtf_features[\"method\"] = dtf_features[[\"p_value\",\"regularization\"]].apply(lambda x: (x[0]+\" \"+x[1]).strip(), axis=1)\n",
    "dtf_features[\"selection\"] = dtf_features[\"num1\"] + dtf_features[\"num2\"]\n",
    "dtf_features[\"method\"] = dtf_features[\"method\"].apply(lambda x: \"both\" if len(x.split()) == 2 else x)\n",
    "sns.barplot(y=\"features\", x=\"selection\", hue=\"method\", data=dtf_features.sort_values(\"selection\", ascending=False), dodge=False, palette=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per avere un'altra idea dell'importanza delle features possiamo utilizzare i metodi di ensable che ci permettono di capire quali sono le featurs che bisogna selezionare. Questi metodi utilizzano modelli multipli per avere in uscita una migliore prestazione predittiva rispetto ai modelli presi singolarmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dtf_train.drop(\"Y\", axis=1).values\n",
    "y = dtf_train[\"Y\"].values\n",
    "feature_names = dtf_train.drop(\"Y\", axis=1).columns.tolist()\n",
    "## call model\n",
    "model = ensemble.GradientBoostingRegressor()\n",
    "## Importance\n",
    "model.fit(X,y)\n",
    "importances = model.feature_importances_\n",
    "## Put in a pandas dtf\n",
    "dtf_importances = pd.DataFrame({\"IMPORTANCE\":importances, \n",
    "            \"VARIABLE\":feature_names}).sort_values(\"IMPORTANCE\", \n",
    "            ascending=False)\n",
    "dtf_importances['cumsum'] = dtf_importances['IMPORTANCE'].cumsum(axis=0)\n",
    "dtf_importances = dtf_importances.set_index(\"VARIABLE\")\n",
    "    \n",
    "## Plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False)\n",
    "fig.suptitle(\"Features Importance\", fontsize=20)\n",
    "ax[0].title.set_text('variables')\n",
    "dtf_importances[[\"IMPORTANCE\"]].sort_values(by=\"IMPORTANCE\").plot(\n",
    "                kind=\"barh\", legend=False, ax=ax[0]).grid(axis=\"x\")\n",
    "ax[0].set(ylabel=\"\")\n",
    "ax[1].title.set_text('cumulative')\n",
    "dtf_importances[[\"cumsum\"]].plot(kind=\"line\", linewidth=4, \n",
    "                                 legend=False, ax=ax[1])\n",
    "ax[1].set(xlabel=\"\", xticks=np.arange(len(dtf_importances)), \n",
    "          xticklabels=dtf_importances.index)\n",
    "plt.xticks(rotation=70)\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selezioniamo quindi le features di nostro interesse dati anche i risultati delle analisi fatte in precedenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_names = ['OverallQual', 'GrLivArea', 'MSSubClass', \"GarageCars\", \"TotalBsmtSF\"]\n",
    "X_train = dtf_train[X_names].values\n",
    "y_train = dtf_train[\"Y\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL DESIGN\n",
    "Andiamo a selzionare un semplice modello regressivo e ad utilizzare come metrica __R squared__. Per la validazione invece abbiamo utilizzato un metodo chiamato K-fold, che va a splittare il dataset in n sotto cartelle che vengono utilizzate per il train e per il test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## call model\n",
    "model = linear_model.LinearRegression()\n",
    "## K fold validation\n",
    "scores = []\n",
    "cv = model_selection.KFold(n_splits=10, shuffle=True)\n",
    "fig = plt.figure()\n",
    "i = 1\n",
    "for train, test in cv.split(X_train, y_train):\n",
    "    prediction = model.fit(X_train[train],\n",
    "                 y_train[train]).predict(X_train[test])\n",
    "    true = y_train[test]\n",
    "    score = metrics.r2_score(true, prediction)\n",
    "    scores.append(score)\n",
    "    plt.scatter(prediction, true, lw=2, alpha=0.3, \n",
    "                label='Fold %d (R2 = %0.2f)' % (i,score))\n",
    "    i = i+1\n",
    "plt.plot([min(y_train),max(y_train)], [min(y_train),max(y_train)], \n",
    "         linestyle='--', lw=2, color='white')\n",
    "print(50*\"-\")\n",
    "print(\"MEAN R2 scores {} with linear regression: \".format(mean(scores)))\n",
    "print(50*\"-\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('K-Fold Validation')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_boost = ensemble.GradientBoostingRegressor()\n",
    "## K fold validation\n",
    "scores = []\n",
    "cv = model_selection.KFold(n_splits=10, shuffle=True)\n",
    "fig = plt.figure()\n",
    "i = 1\n",
    "for train, test in cv.split(X_train, y_train):\n",
    "    prediction = model_boost.fit(X_train[train],\n",
    "                 y_train[train]).predict(X_train[test])\n",
    "    true = y_train[test]\n",
    "    score = metrics.r2_score(true, prediction)\n",
    "    scores.append(score)\n",
    "    plt.scatter(prediction, true, lw=2, alpha=0.3, \n",
    "                label='Fold %d (R2 = %0.2f)' % (i,score))\n",
    "    i = i+1\n",
    "plt.plot([min(y_train),max(y_train)], [min(y_train),max(y_train)], \n",
    "         linestyle='--', lw=2, color='white')\n",
    "print(50*\"-\")\n",
    "print(\"MEAN R2 scores {} with Gradient Boost: \".format(mean(scores)))\n",
    "print(50*\"-\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('K-Fold Validation')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo notare il gradient boost ha una performance migliore nella predizione quindi da questo punto in poi porteremo lui avanti come modello per andare a predirre i prezzi dal datset di test. Quindi andiamo a ripetere il processo di preprocess sul nostro dataset di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dtf_test.columns.to_list()\n",
    "for el in features:\n",
    "    s = dtf_test[el].isna().sum()\n",
    "    percent = round((s*100)/(len(dtf_test.index)),2)\n",
    "    print(\"la colonna {} ha {} NaN [{} %]\".format(el,s,percent))\n",
    "\n",
    "dtf_test[\"LotFrontage\"] = dtf_test[\"LotFrontage\"].fillna(dtf_test[\"LotFrontage\"].mean())\n",
    "dtf_test.head()\n",
    "\n",
    "\n",
    "scalerX = preprocessing.RobustScaler(quantile_range=(25.0, 75.0))\n",
    "X = scalerX.fit_transform(dtf_test.drop(\"Y\", axis=1))\n",
    "dtf_scaled_test= pd.DataFrame(X, columns=dtf_test.drop(\"Y\", \n",
    "                        axis=1).columns, index=dtf_test.index)\n",
    "## scale Y\n",
    "scalerY = preprocessing.RobustScaler(quantile_range=(25.0, 75.0))\n",
    "dtf_scaled_test[\"Y\"] = scalerY.fit_transform(dtf_test[\"Y\"].values.reshape(-1,1))\n",
    "dtf_scaled_test.head()\n",
    "\n",
    "\n",
    "X_test = dtf_train[X_names].values\n",
    "y_test = dtf_train[\"Y\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo ad effettuare quindi la predizione utilizzando come modello quello trainati con il gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model_boost.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utilizziamo come metriche di paragone del modello l' R_squared e successivamente alcuni errori come RMSE e MAE che ci permettono di avere informazioni relative alla qualità totale della regressione che abbiamo implementato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kpi\n",
    "print(\"R2 (explained variance):\", round(metrics.r2_score(y_test, predicted), 2))\n",
    "print(\"Mean Absolute Perc Error (Σ(|y-pred|/y)/n):\", round(np.mean(np.abs((y_test-predicted)/predicted)), 2))\n",
    "print(\"Mean Absolute Error (Σ|y-pred|/n):\", \"{:,.0f}\".format(metrics.mean_absolute_error(y_test, predicted)))\n",
    "print(\"Root Mean Squared Error (sqrt(Σ(y-pred)^2/n)):\", \"{:,.0f}\".format(np.sqrt(metrics.mean_squared_error(y_test, predicted))))\n",
    "## residuals\n",
    "residuals = y_test - predicted\n",
    "max_error = max(residuals) if abs(max(residuals)) > abs(min(residuals)) else min(residuals)\n",
    "max_idx = list(residuals).index(max(residuals)) if abs(max(residuals)) > abs(min(residuals)) else list(residuals).index(min(residuals))\n",
    "max_true, max_pred = y_test[max_idx], predicted[max_idx]\n",
    "print(\"Max Error:\", \"{:,.0f}\".format(max_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot predicted vs true\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "ax[0].scatter(predicted, y_test, color=\"olive\")\n",
    "abline_plot(intercept=0, slope=1, color=\"white\", ax=ax[0])\n",
    "ax[0].vlines(x=max_pred, ymin=max_true, ymax=max_true-max_error, color='white', linestyle='--', alpha=0.7, label=\"max error\")\n",
    "ax[0].grid(True, c=\"teal\")\n",
    "ax[0].set(xlabel=\"Predicted\", ylabel=\"True\", title=\"Predicted vs True\")\n",
    "ax[0].legend()\n",
    "    \n",
    "## Plot predicted vs residuals\n",
    "ax[1].scatter(predicted, residuals, color=\"olive\")\n",
    "ax[1].vlines(x=max_pred, ymin=0, ymax=max_error, color='white', linestyle='--', alpha=0.7, label=\"max error\")\n",
    "ax[1].grid(True, c=\"teal\")\n",
    "ax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")\n",
    "ax[1].hlines(y=0, xmin=np.min(predicted), xmax=np.max(predicted))\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vediamo da questi plot come il massimo errore sia di circa 216k, mentre i residui sembrano concentrarsi su un valore che oscilla intorno ai 50k, andiamo però ad osservarne la distribuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.distplot(residuals, color=\"olive\",hist=True, kde=True, kde_kws={\"shade\":True}, ax=ax)\n",
    "ax.grid(True, c=\"teal\")\n",
    "ax.set(yticks=[], yticklabels=[], title=\"Residuals distribution\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07e5a5eff9b258874276a6199eba70556246eea104cbf47c84a0e9033d5ce69c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('AI_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
